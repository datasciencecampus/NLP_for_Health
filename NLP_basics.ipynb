{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLP basics\n",
    "#Tokenization – process of converting a text into tokens\n",
    "#Tokens – words or entities present in the text\n",
    "#Text object – a sentence or a phrase or a word or an article\n",
    "\n",
    "\n",
    "\n",
    "#Table of Contents\n",
    "\n",
    "#Stemming\n",
    "#Lemmatisation\n",
    "#Word Embeddings\n",
    "#Part-of-Speech Tagging\n",
    "#Named Entity Disambiguation\n",
    "#Named Entity Recognition\n",
    "#Sentiment Analysis\n",
    "#Semantic Text Similarity\n",
    "#Language Identification\n",
    "#Text Summarisation\n",
    "\n",
    "import nltk \n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Noise Removal\n",
    "#Lexicon Normalization\n",
    "#Object Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'at the sample text'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Noise Removal\n",
    "\n",
    "# Any piece of text which is not relevant to the context\n",
    "#of the data and the end-output can be specified as the noise.\n",
    "#For example – language stopwords (commonly used words of a language\n",
    "#– is, am, the, of, in etc), URLs or links, social media entities\n",
    "#(mentions, hashtags), punctuations and industry specific words. \n",
    "#This step deals with removal of all types of noisy entities present in the text.\n",
    "#A general approach for noise removal is to prepare a dictionary of noisy\n",
    "#entities, and iterate the text object by tokens (or by words), eliminating\n",
    "#those tokens which are present in the noise dictionary.\n",
    "noise_list = [\"is\", \"a\", \"this\", \"...\"] \n",
    "def _remove_noise(input_text):\n",
    "    words = input_text.split() \n",
    "    noise_free_words = [word for word in words if word not in noise_list] \n",
    "    noise_free_text = \" \".join(noise_free_words) \n",
    "    return noise_free_text\n",
    "_remove_noise(\"this is a at the ... sample text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remove this  from this website'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample code to remove a regex pattern \n",
    "import re\n",
    "def _remove_regex(input_text, regex_pattern):\n",
    "    urls = re.finditer(regex_pattern, input_text) \n",
    "    for i in urls: \n",
    "        input_text = re.sub(i.group().strip(), '', input_text)\n",
    "    return input_text\n",
    "\n",
    "regex_pattern = \"#[\\w]*\" \n",
    "\n",
    "_remove_regex(\"remove this #hashtag from this website\", regex_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lexicon Normalization\n",
    "\n",
    "\n",
    "#Normalization is a pivotal step for feature \n",
    "#engineering with text as it converts the high \n",
    "#dimensional features (N different features)\n",
    "#to the low dimensional space (1 feature), \n",
    "#which is an ideal ask for any ML model.\n",
    "\n",
    "#Stemming:  \n",
    "#Stemming is a rudimentary rule-based process of\n",
    "#stripping the suffixes (“ing”, “ly”, “es”, “s” etc) from a word.\n",
    "\n",
    "\n",
    "#Lemmatization: Lemmatization, on the other hand, is an organized &\n",
    "#step by step procedure of obtaining the root form of the word, it \n",
    "#makes use of vocabulary (dictionary importance of words) and\n",
    "#morphological analysis (word structure and grammar relations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer \n",
    "\n",
    "# Defining objects \n",
    "lem = WordNetLemmatizer()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer \n",
    "# Defining objects\n",
    "stem = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "integrate\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"multiplying\" \n",
    "word_2 = \"integrating\"\n",
    "print lem.lemmatize(word_1, \"v\")\n",
    "print lem.lemmatize(word_2, \"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multipli\n",
      "integr\n"
     ]
    }
   ],
   "source": [
    "word_1 = \"multiplying\" \n",
    "word_2 = \"integrating\"\n",
    "print stem.stem(word_1)\n",
    "print stem.stem(word_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is a Retweet retweeted tweet by me and its awesome'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Object Standardization\n",
    "text_raw =\"This is a rt retweeted tweet by me and its awsm\"\n",
    "\n",
    "lookup_dict = {'rt':'Retweet', 'dm':'direct message', \"awsm\" : \"awesome\", \"luv\" :\"love\"}\n",
    "def _lookup_words(input_text):\n",
    "    words = input_text.split() \n",
    "    new_words = [] \n",
    "    for word in words:\n",
    "        if word.lower() in lookup_dict:\n",
    "            word = lookup_dict[word.lower()]\n",
    "        new_words.append(word) \n",
    "        new_text = \" \".join(new_words) \n",
    "    return new_text\n",
    "\n",
    "_lookup_words(text_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')]\n"
     ]
    }
   ],
   "source": [
    "# Text to Features\n",
    "# Syntactic Parsing\n",
    "from nltk import word_tokenize, pos_tag\n",
    "text = \"I am learning Natural Language Processing on Analytics Vidhya\"\n",
    "tokens = word_tokenize(text)\n",
    "print pos_tag(tokens)\n",
    "## Proper Noun:\n",
    "#NNP (Proper Noun)\n",
    "# VBG Verb gerund \n",
    "\n",
    "setence_1 = \"Please book my flight for Delhi\"\n",
    "\n",
    "setence_2 = \"I am going to read this book in the flight\"\n",
    "\n",
    "\n",
    "# Context based ?\n",
    "\n",
    "pos_tag(word_tokenize(setence_2))\n",
    "\n",
    "pos_tag(word_tokenize(setence_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The process of detecting the named entities \n",
    "#such as person names, location names, company \n",
    "#names etc from the text is called as NER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Sugar',\n",
       "  'is',\n",
       "  'bad',\n",
       "  'to',\n",
       "  'consume.',\n",
       "  'My',\n",
       "  'sister',\n",
       "  'likes',\n",
       "  'to',\n",
       "  'have',\n",
       "  'sugar,',\n",
       "  'but',\n",
       "  'not',\n",
       "  'my',\n",
       "  'father.'],\n",
       " ['My',\n",
       "  'father',\n",
       "  'spends',\n",
       "  'a',\n",
       "  'lot',\n",
       "  'of',\n",
       "  'time',\n",
       "  'driving',\n",
       "  'my',\n",
       "  'sister',\n",
       "  'around',\n",
       "  'to',\n",
       "  'dance',\n",
       "  'practice.'],\n",
       " ['Doctors',\n",
       "  'suggest',\n",
       "  'that',\n",
       "  'driving',\n",
       "  'may',\n",
       "  'cause',\n",
       "  'increased',\n",
       "  'stress',\n",
       "  'and',\n",
       "  'blood',\n",
       "  'pressure.']]"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Topic modeling is a process of automatically \n",
    "#identifying the topics present in a text corpus, \n",
    "#it derives the hidden patterns among the words \n",
    "#in the corpus in an unsupervised manner. \n",
    "\n",
    "\n",
    "#Topics are defined as “a repeating pattern of co-occurring \n",
    "#terms in a corpus”. A good topic model results in – “health”, \n",
    "#“doctor”, “patient”, “hospital” for a topic – Healthcare, \n",
    "#and “farm”, “crops”, “wheat” for a topic – “Farming”.\n",
    "\n",
    "\n",
    "# Latent Dirichlet Allocation (LDA)\n",
    "\n",
    "\n",
    "\n",
    "doc1 = \"Sugar is bad to consume. My sister likes to have sugar, but not my father.\" \n",
    "doc2 = \"My father spends a lot of time driving my sister around to dance practice.\"\n",
    "doc3 = \"Doctors suggest that driving may cause increased stress and blood pressure.\"\n",
    "doc_complete = [doc1, doc2, doc3]\n",
    "doc_clean = [doc.split() for doc in doc_complete]\n",
    "doc_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "## https://radimrehurek.com/gensim/index.html\n",
    "\n",
    "#Gensim is a free Python library designed to \n",
    "#automatically extract semantic topics \n",
    "#from documents, as efficiently (computer-wise) \n",
    "#and painlessly (human-wise) as possible.\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 2)], [(0, 1), (9, 1), (11, 1), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1)], [(17, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)]]\n",
      "[(0, u'0.029*\"my\" + 0.029*\"sister\" + 0.029*\"My\" + 0.029*\"to\" + 0.029*\"increased\" + 0.029*\"stress\" + 0.029*\"Doctors\" + 0.029*\"suggest\" + 0.029*\"and\" + 0.029*\"may\"'), (1, u'0.064*\"driving\" + 0.037*\"lot\" + 0.037*\"a\" + 0.037*\"around\" + 0.037*\"time\" + 0.037*\"dance\" + 0.037*\"father\" + 0.037*\"of\" + 0.037*\"practice.\" + 0.037*\"spends\"'), (2, u'0.089*\"to\" + 0.051*\"My\" + 0.051*\"sister\" + 0.051*\"my\" + 0.051*\"sugar,\" + 0.051*\"not\" + 0.051*\"Sugar\" + 0.051*\"is\" + 0.051*\"bad\" + 0.051*\"have\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the term dictionary of our corpus, where every unique term is assigned an index.  \n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "\n",
    "#Converting list of documents (corpus) into Document\n",
    "#Term Matrix using dictionary prepared above. \n",
    "## ?? vectors number of times each elements appears\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "print (doc_term_matrix)\n",
    "# Creating the object for LDA model using gensim library\n",
    "Lda = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "# Results \n",
    "print(ldamodel.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, u'0.200*\"He\" + 0.200*\"boy\" + 0.200*\"is\" + 0.200*\"a\" + 0.200*\"big\"')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text=\"He is a big big boy\"\n",
    "doc_clean= [text.split()]\n",
    "dictionary = corpora.Dictionary(doc_clean)\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Running and Training LDA model on the document term matrix\n",
    "ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)\n",
    "# Results \n",
    "print(ldamodel.print_topics()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['this', 'is'], ['is', 'a'], ['a', 'sample'], ['sample', 'text']]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N-Grams as Features\n",
    "def generate_ngrams(text, n):\n",
    "    words = text.split()\n",
    "    output = []  \n",
    "    for i in range(len(words)-n+1):\n",
    "        output.append(words[i:i+n])\n",
    "    return output\n",
    "\n",
    "# Binary gram \n",
    "generate_ngrams('this is a sample text', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 7)\t0.5844829010200651\n",
      "  (0, 2)\t0.5844829010200651\n",
      "  (0, 4)\t0.444514311537431\n",
      "  (0, 1)\t0.34520501686496574\n",
      "  (1, 1)\t0.3853716274664007\n",
      "  (1, 0)\t0.652490884512534\n",
      "  (1, 3)\t0.652490884512534\n",
      "  (2, 4)\t0.444514311537431\n",
      "  (2, 1)\t0.34520501686496574\n",
      "  (2, 6)\t0.5844829010200651\n",
      "  (2, 5)\t0.5844829010200651\n"
     ]
    }
   ],
   "source": [
    "#Term Frequency (TF) – TF for a term “t” is defined \n",
    "#as the count of a term “t” in a document “D”\n",
    "\n",
    "#Inverse Document Frequency (IDF) – IDF for a term \n",
    "#is defined as logarithm of ratio of total documents\n",
    "#available in the corpus and number of documents \n",
    "#containing the term T.\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "obj = TfidfVectorizer()\n",
    "obj = TfidfVectorizer()\n",
    "corpus = ['This is sample document.', 'another random document.', 'third sample document text']\n",
    "X = obj.fit_transform(corpus)\n",
    "print X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.2366132946433113\n",
      "[ 2.5001867e-03 -4.5279837e-03  3.2164674e-04 -4.4285697e-03\n",
      "  4.1545318e-03 -2.1649762e-03 -3.8046767e-03 -1.7539723e-03\n",
      "  5.9814251e-04  3.5325827e-03 -3.9156429e-03  5.8388693e-04\n",
      "  2.1662123e-03 -4.5341258e-03  1.5721378e-03  4.6008709e-03\n",
      " -2.1712466e-03 -1.1091464e-03 -3.1049079e-03 -3.8329777e-03\n",
      "  2.6114439e-03  6.9665466e-04  1.8230440e-03 -2.0439368e-04\n",
      " -1.3953244e-03  2.5326526e-03 -1.3467000e-03  7.9407363e-04\n",
      " -8.5678376e-04 -2.7677296e-03 -3.2886763e-03 -4.6619233e-03\n",
      "  7.3562146e-06  1.1412736e-03 -1.2158153e-03  7.6474954e-04\n",
      " -4.3854597e-03  1.7202114e-03  2.6836181e-03  1.4622611e-03\n",
      " -2.5849789e-03  4.8951223e-03  4.8026382e-03 -1.6887399e-03\n",
      "  3.7905199e-03 -1.8123538e-03 -1.8461549e-03 -3.6006782e-03\n",
      " -8.0820097e-04  2.4030278e-03  2.6205764e-03 -1.3611730e-03\n",
      "  3.2463272e-03  4.4065714e-03 -4.9645538e-03  1.0241962e-03\n",
      "  7.2724192e-04  1.0305879e-03  3.8043426e-03 -1.5256479e-03\n",
      "  4.4337765e-04  8.2238915e-04  1.0542222e-03  1.5848925e-04\n",
      "  2.7174465e-04 -4.1874815e-03 -1.3866873e-03 -4.4830837e-03\n",
      " -5.5923843e-04 -4.6276394e-03 -4.6137613e-03  1.1266705e-03\n",
      "  2.8460119e-03 -4.0153079e-03 -1.0213936e-03 -3.4116728e-03\n",
      " -4.3979678e-03 -2.1404165e-03  4.8894719e-03  1.2435411e-03\n",
      " -2.9593618e-03  6.9390453e-04 -1.7993891e-04  3.9033850e-03\n",
      "  3.8551667e-03 -3.4354749e-04 -1.7333762e-03 -3.4099706e-03\n",
      "  2.2529550e-03  7.4087334e-04  9.8270993e-04 -2.9371216e-04\n",
      " -2.5866383e-03  3.2008286e-03 -3.1004008e-03  4.4333171e-03\n",
      "  4.8331725e-03  1.0723709e-03 -5.6745659e-04 -2.0097558e-04]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:19: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:21: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    }
   ],
   "source": [
    "#Word Embedding (text vectors)\n",
    "#Word2Vec\n",
    "#GloVe \n",
    "\n",
    "#Word2Vec model is composed of preprocessing module, a shallow \n",
    "#neural network model called Continuous Bag of Words and another\n",
    "#shallow neural network model called skip-gram. These models are \n",
    "#widely used for all other nlp problems. It first constructs a vocabulary\n",
    "#from the training corpus and then learns word embedding representations. \n",
    "#Following code using gensim package prepares the word embedding as the vector\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "sentences = [['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],\n",
    "             ['machine', 'learning'], ['deep', 'learning']]\n",
    "\n",
    "\n",
    "# train the model on your corpus\n",
    "model = Word2Vec(sentences, min_count = 1)\n",
    "print model.similarity('data', 'science')\n",
    "\n",
    "print model['learning'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class_A\n",
      "Class_B\n",
      "Class_A\n",
      "0.833333333333\n"
     ]
    }
   ],
   "source": [
    "## Important tasks of NLP\n",
    "# Text Classification\n",
    "\n",
    "#Email Spam Identification, topic classification of news,\n",
    "#sentiment classification and organization of web pages by search engines.\n",
    "\n",
    "\n",
    "#Text classification, in common words is defined as a technique to systematically\n",
    "#classify a text object (document or sentence) in one of the fixed category. \n",
    "#It is really helpful when the amount of data is too large, especially for organizing, \n",
    "#information filtering, and storage purposes.\n",
    "#A typical natural language classifier consists of two parts: \n",
    "#(a) Training (b) Prediction as shown in image below. Firstly the \n",
    "#text input is processes and features are created. The machine learning models\n",
    "#then learn these features and is used for predicting against the new text.\n",
    "\n",
    "\n",
    "\n",
    "# Installing\n",
    "#pip install -U textblob\n",
    "#python -m textblob.download_corpora\n",
    "\n",
    "\n",
    "from textblob.classifiers import NaiveBayesClassifier as NBC\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "training_corpus = [ ('I am exhausted of this work.', 'Class_B'),\n",
    "                   (\"I can't cooperate with this\", 'Class_B'),\n",
    "                   ('He is my badest enemy!', 'Class_B'),\n",
    "                   ('My management is poor.', 'Class_B'),\n",
    "                   ('I love this burger.', 'Class_A'),\n",
    "                   ('This is an brilliant place!', 'Class_A'),\n",
    "                   ('I feel very good about these dates.', 'Class_A'),\n",
    "                   ('This is my best work.', 'Class_A'),\n",
    "                   (\"What an awesome view\", 'Class_A'),\n",
    "                   ('I do not like this dish', 'Class_B')]\n",
    "test_corpus = [(\"I am not feeling well today.\", 'Class_B'), \n",
    "                (\"I feel brilliant!\", 'Class_A'), \n",
    "                ('Gary is a friend of mine.', 'Class_A'), \n",
    "                (\"I can't believe I'm doing this.\", 'Class_B'), \n",
    "                ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')]\n",
    "\n",
    "model = NBC(training_corpus) \n",
    "\n",
    "print(model.classify(\"Their codes are amazing.\"))\n",
    "print(model.classify(\"I don't like their computer.\"))\n",
    "\n",
    "print (model.classify(test_corpus[4][0]))\n",
    "print(model.accuracy(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "    Class_A       0.50      0.67      0.57         3\n",
      "    Class_B       0.50      0.33      0.40         3\n",
      "\n",
      "avg / total       0.50      0.50      0.49         6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import svm\n",
    "\n",
    "# preparing data for SVM model (using the same training_corpus, test_corpus from naive bayes example)\n",
    "train_data = []\n",
    "train_labels = []\n",
    "for row in training_corpus:\n",
    "    train_data.append(row[0])\n",
    "    train_labels.append(row[1])\n",
    "\n",
    "test_data = [] \n",
    "test_labels = [] \n",
    "for row in test_corpus:\n",
    "    test_data.append(row[0]) \n",
    "    test_labels.append(row[1])\n",
    "\n",
    "# Create feature vectors \n",
    "vectorizer = TfidfVectorizer(min_df=4, max_df=0.9)\n",
    "# Train the feature vectors\n",
    "train_vectors = vectorizer.fit_transform(train_data)\n",
    "# Apply model on test data \n",
    "test_vectors = vectorizer.transform(test_data)\n",
    "\n",
    "# Perform classification with SVM, kernel=linear \n",
    "model = svm.SVC(kernel='linear') \n",
    "model.fit(train_vectors, train_labels) \n",
    "prediction = model.predict(test_vectors)\n",
    "\n",
    "\n",
    "print (classification_report(test_labels, prediction))\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Matching / Similarity\n",
    "#A. Levenshtein Distance – The Levenshtein distance between \n",
    "#two strings is defined as the minimum number of edits needed to \n",
    "#transform one string into the other, with the allowable edit \n",
    "#operations being insertion, deletion, or\n",
    "#substitution of a single character. Following is the implementation \n",
    "#for efficient memory computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "def levenshtein(s1,s2): \n",
    "    if len(s1) > len(s2):\n",
    "        s1,s2 = s2,s1 \n",
    "    distances = range(len(s1) + 1) \n",
    "    for index2,char2 in enumerate(s2):\n",
    "        newDistances = [index2+1]\n",
    "        for index1,char1 in enumerate(s1):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1]) \n",
    "            else:\n",
    "                 newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) \n",
    "        distances = newDistances \n",
    "    return distances[-1]\n",
    "\n",
    "print(levenshtein(\"analyze\",\"analyse\"))\n",
    "\n",
    "print levenshtein(\"analyze\",\"information\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Phonetic Matching – A Phonetic matching algorithm takes a \n",
    "#keyword as input (person’s name, location name etc) and produces \n",
    "#a character string that identifies a set of words that are (roughly)\n",
    "#phonetically similar. It is very useful for searching large text corpuses,\n",
    "#correcting spelling errors and matching relevant names. Soundex and Metaphone\n",
    "#are two main phonetic algorithms used for this purpose. Python’s module Fuzzy\n",
    "#is used to compute soundex strings for different words, for example –\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from fuzzywuzzy import process\n",
    "fuzz.ratio(\"ACME Factory alpha\", \"ACME Factory Inc.\")\n",
    "fuzz.partial_ratio(\"ACME Factory alpha\", \"ACME Factory Inc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A523\n",
      "A523\n"
     ]
    }
   ],
   "source": [
    "import fuzzy\n",
    "soundex = fuzzy.Soundex(4)\n",
    "print soundex('ankita')\n",
    "print soundex('aunkit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
